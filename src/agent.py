import os
from typing import Annotated, Sequence, TypedDict, Literal, List
from operator import add
import numpy as np

from langchain_core.messages import BaseMessage, ToolMessage, SystemMessage, HumanMessage, AIMessage
from langchain_ollama import ChatOllama
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.tools import tool
from dotenv import load_dotenv

# Bibliotecas para MongoDB
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure

# Bibliotecas para busca na internet
import requests
from bs4 import BeautifulSoup
from googlesearch import search

# Biblioteca para embeddings especializados
from sentence_transformers import SentenceTransformer

# Carrega vari√°veis de ambiente
load_dotenv()

# =============== CONFIGURA√á√ÉO DO MONGODB ===============

# Conex√£o com MongoDB (local por padr√£o, pode ser Atlas)
MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017/")
DATABASE_NAME = "qa_system"
COLLECTION_NAME = "resumos_conhecimento"

def get_mongo_collection():
    """Retorna a cole√ß√£o do MongoDB."""
    try:
        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
        # Testa a conex√£o
        client.admin.command('ping')
        db = client[DATABASE_NAME]
        return db[COLLECTION_NAME]
    except ConnectionFailure as e:
        print(f"Erro ao conectar ao MongoDB: {e}")
        return None

# =============== EMBEDDINGS ===============

# Classe wrapper para Sentence-BERT (compat√≠vel com a interface anterior)
class SentenceBertEmbeddings:
    """
    Wrapper para Sentence-BERT que fornece embeddings de alta qualidade.
    
    Vantagens sobre OllamaEmbeddings:
    - Scores de similaridade mais confi√°veis (0.6-0.9 para relacionados)
    - Modelo especializado em embeddings (n√£o generalista como llama3.1)
    - Mais r√°pido e menor (420MB vs 4.5GB)
    - Suporta portugu√™s atrav√©s do modelo multilingual
    """
    
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        """
        Inicializa o modelo de embeddings.
        
        Modelos recomendados:
        - 'paraphrase-multilingual-MiniLM-L12-v2': Multilingual, balanceado (420MB)
        - 'paraphrase-multilingual-mpnet-base-v2': Multilingual, mais preciso (1GB)
        - 'all-MiniLM-L6-v2': Ingl√™s, mais r√°pido (80MB)
        """
        print(f"üîÑ Carregando modelo de embeddings: {model_name}")
        self.model = SentenceTransformer(model_name)
        print(f"‚úÖ Modelo carregado com sucesso!")
    
    def embed_query(self, text: str) -> list:
        """Gera embedding para um texto (compat√≠vel com OllamaEmbeddings)."""
        return self.model.encode(text).tolist()

# Inicializa o modelo de embeddings especializado
# Nota: Primeira execu√ß√£o baixa o modelo (~420MB)
embeddings_model = SentenceBertEmbeddings()

def calcular_similaridade_coseno(vec1: List[float], vec2: List[float]) -> float:
    """Calcula similaridade de cosseno entre dois vetores."""
    vec1_np = np.array(vec1)
    vec2_np = np.array(vec2)
    
    dot_product = np.dot(vec1_np, vec2_np)
    norm1 = np.linalg.norm(vec1_np)
    norm2 = np.linalg.norm(vec2_np)
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

# =============== DEFINI√á√ÉO DO STATE ===============

class AgentState(TypedDict):
    """Estado do agente contendo mensagens."""
    messages: Annotated[Sequence[BaseMessage], add]

# =============== FERRAMENTAS (@tool) ===============

@tool
def consultar_BD(pergunta: str) -> str:
    """
    Consulta a base de dados MongoDB usando embeddings para encontrar os 3 resumos mais similares.
    
    Args:
        pergunta: A pergunta do usu√°rio para buscar no banco
        
    Returns:
        String com os 3 resumos mais similares encontrados (k=3) OU mensagem indicando que n√£o h√° informa√ß√£o relevante
    """
    try:
        collection = get_mongo_collection()
        if collection is None:
            return "Erro: N√£o foi poss√≠vel conectar ao banco de dados."
        
        # Gera embedding da pergunta
        pergunta_embedding = embeddings_model.embed_query(pergunta)
        
        # Busca todos os documentos que t√™m embeddings
        documentos = list(collection.find({"embedding": {"$exists": True}}))
        
        if not documentos:
            return "‚ùå BANCO DE DADOS VAZIO: N√£o h√° resumos salvos. Use buscar_referencias para buscar na internet."
        
        # Calcula similaridade para cada documento
        similaridades = []
        for doc in documentos:
            similaridade = calcular_similaridade_coseno(
                pergunta_embedding, 
                doc['embedding']
            )
            similaridades.append((doc, similaridade))
        
        # Ordena por similaridade (maior para menor) e pega top 3
        similaridades.sort(key=lambda x: x[1], reverse=True)
        top_3 = similaridades[:3]
        
        # Verifica o melhor score
        melhor_score = top_3[0][1]
        
        # Threshold de similaridade para Sentence-BERT: 0.6 (60%)
        # Sentence-BERT gera scores bem separados:
        #   - Perguntas relacionadas: ~0.65-0.95 (alta confian√ßa)
        #   - Perguntas n√£o relacionadas: ~0.15-0.45 (baixa confian√ßa)
        # Threshold de 0.6 separa perfeitamente os dois grupos!
        THRESHOLD = 0.6
        
        # Se o melhor resultado tem similaridade < threshold, n√£o √© relevante
        if melhor_score < THRESHOLD:
            resultado = f"‚ùå SIMILARIDADE BAIXA (melhor: {melhor_score:.3f} < {THRESHOLD}):\n\n"
            resultado += "Os resumos no banco N√ÉO s√£o relevantes para esta pergunta:\n\n"
            
            for idx, (doc, score) in enumerate(top_3, 1):
                resultado += f"{idx}. [Score: {score:.3f}] Tema: {doc.get('tema', 'Sem tema')}\n"
            
            resultado += "\n‚ö†Ô∏è A√á√ÉO NECESS√ÅRIA: Use buscar_referencias para buscar informa√ß√µes na internet."
            return resultado
        
        # Se chegou aqui, tem resultados relevantes (score >= threshold)
        resultado = f"‚úÖ ENCONTRADO NO BANCO (melhor similaridade: {melhor_score:.3f} >= {THRESHOLD}):\n\n"
        resultado += "Os seguintes resumos S√ÉO RELEVANTES para responder a pergunta:\n\n"
        
        for idx, (doc, score) in enumerate(top_3, 1):
            if score >= THRESHOLD:  # S√≥ mostra os realmente relevantes
                resultado += f"{idx}. [Similaridade: {score:.3f}] ‚úÖ RELEVANTE\n"
                resultado += f"   Tema: {doc.get('tema', 'Sem tema')}\n"
                resultado += f"   Resumo: {doc['resumo'][:300]}...\n"
                resultado += f"   Fontes: {doc.get('fontes', 'N/A')}\n"
                resultado += f"   {'-'*60}\n\n"
        
        resultado += "\n‚úÖ A√á√ÉO: Use estas informa√ß√µes para responder ao usu√°rio. N√ÉO busque na internet."
        return resultado
            
    except Exception as e:
        return f"Erro ao consultar banco de dados: {str(e)}"


@tool
def buscar_referencias(query: str) -> str:
    """
    Busca refer√™ncias na internet usando Google Search e retorna o conte√∫do do primeiro link.
    
    Args:
        query: Consulta para buscar no Google
        
    Returns:
        String com o conte√∫do extra√≠do do primeiro resultado
    """
    try:
        # Busca no Google (pega apenas o primeiro resultado)
        search_results = list(search(query, num_results=1, lang="pt"))
        
        if not search_results:
            return "Nenhum resultado encontrado na busca."
        
        url = search_results[0]
        
        # Faz requisi√ß√£o para o primeiro link
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # Extrai o texto da p√°gina
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Remove scripts e styles
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Pega o texto
        texto = soup.get_text()
        
        # Limpa o texto (remove linhas vazias e espa√ßos extras)
        linhas = (line.strip() for line in texto.splitlines())
        chunks = (phrase.strip() for line in linhas for phrase in line.split("  "))
        texto_limpo = ' '.join(chunk for chunk in chunks if chunk)
        
        # Limita o tamanho (primeiros 2000 caracteres)
        texto_resumido = texto_limpo[:2000]
        
        return f"""üåê Informa√ß√µes encontradas na internet:

URL: {url}
Conte√∫do: {texto_resumido}..."""
        
    except Exception as e:
        return f"Erro ao buscar na internet: {str(e)}"


@tool
def atualizar_BD(tema: str, resumo: str, fontes: str) -> str:
    """
    Salva um resumo de conhecimento no banco de dados com seu embedding para busca futura.
    
    Args:
        tema: O tema/t√≥pico principal do resumo
        resumo: O resumo consolidado da conversa
        fontes: As fontes de informa√ß√£o utilizadas (URLs, etc)
        
    Returns:
        Mensagem de confirma√ß√£o
    """
    try:
        collection = get_mongo_collection()
        if collection is None:
            return "Erro: N√£o foi poss√≠vel conectar ao banco de dados."
        
        # Gera embedding do resumo para busca futura por similaridade
        resumo_embedding = embeddings_model.embed_query(resumo)
        
        # Insere documento com embedding
        documento = {
            "tema": tema,
            "resumo": resumo,
            "fontes": fontes,
            "embedding": resumo_embedding
        }
        
        resultado = collection.insert_one(documento)
        
        return f"‚úÖ Resumo sobre '{tema}' salvo com sucesso! ID: {resultado.inserted_id}"
            
    except Exception as e:
        return f"Erro ao salvar no banco de dados: {str(e)}"


@tool
def gerar_resumo(tema: str) -> str:
    """
    Gera um resumo consolidado da conversa ATUAL usando o LLM.
    Esta ferramenta analisa o hist√≥rico de mensagens e cria um resumo inteligente.
    DEPOIS de gerar o resumo, use atualizar_BD para salv√°-lo.
    
    Args:
        tema: O tema principal da conversa atual
        
    Returns:
        O resumo gerado pelo LLM
    """
    try:
        # Esta fun√ß√£o retorna apenas uma flag indicando que deve processar
        # O agente ir√° pegar o hist√≥rico do estado e gerar o resumo
        return f"GERAR_RESUMO:{tema}"
        
    except Exception as e:
        return f"Erro ao gerar resumo: {str(e)}"


# =============== CONFIGURA√á√ÉO DO MODELO E TOOLS ===============

tools = [consultar_BD, buscar_referencias, atualizar_BD, gerar_resumo]
# llama3.1 e llama3.2 suportam function calling (tools)
model = ChatOllama(model="llama3.1", temperature=0.7).bind_tools(tools)

# =============== NODES DO GRAFO ===============

def agent_node(state: AgentState) -> AgentState:
    """
    N√≥ principal do agente que processa mensagens e decide a√ß√µes (padr√£o ReACT).
    """
    
    # Verifica se precisa gerar resumo da conversa
    last_message = state["messages"][-1]
    if isinstance(last_message, ToolMessage) and "GERAR_RESUMO:" in last_message.content:
        tema = last_message.content.replace("GERAR_RESUMO:", "")
        
        # Extrai hist√≥rico da conversa para resumir
        historico = ""
        fontes_usadas = []
        
        for msg in state["messages"]:
            if isinstance(msg, HumanMessage):
                historico += f"\nüë§ Usu√°rio: {msg.content}\n"
            elif isinstance(msg, AIMessage):
                historico += f"\nü§ñ Assistente: {msg.content}\n"
            elif isinstance(msg, ToolMessage):
                # Extrai fontes (URLs) das buscas
                if "URL:" in msg.content:
                    import re
                    urls = re.findall(r'URL: (https?://[^\s]+)', msg.content)
                    fontes_usadas.extend(urls)
        
        # Gera resumo com LLM
        resumo_prompt = f"""Analise a conversa abaixo sobre o tema "{tema}" e crie um resumo consolidado.

CONVERSA:
{historico}

Crie um resumo estruturado contendo:
1. Principais perguntas feitas
2. Respostas e conhecimentos adquiridos  
3. Conceitos-chave explicados
4. Conclus√µes importantes

Seja claro, objetivo e organize o conhecimento de forma √∫til para consultas futuras."""

        resumo_model = ChatOllama(model="llama3.1", temperature=0.3)
        resumo_response = resumo_model.invoke([HumanMessage(content=resumo_prompt)])
        resumo_gerado = resumo_response.content
        
        # Formata fontes
        fontes_str = ", ".join(set(fontes_usadas)) if fontes_usadas else "Conversa com LLM"
        
        # Retorna resposta com o resumo e instru√ß√£o para salvar
        resposta_final = f"""üìù RESUMO GERADO DA CONVERSA:

{resumo_gerado}

---
Fontes utilizadas: {fontes_str}

Agora vou salvar este resumo no banco de dados para consultas futuras..."""
        
        # Cria mensagem AI com tool_call para salvar
        ai_msg = AIMessage(
            content=resposta_final,
            tool_calls=[{
                "name": "atualizar_BD",
                "args": {
                    "tema": tema,
                    "resumo": resumo_gerado,
                    "fontes": fontes_str
                },
                "id": "resumo_save",
                "type": "tool_call"
            }]
        )
        return {"messages": [ai_msg]}
    
    # Comportamento normal do agente
    system_prompt = SystemMessage(content="""Voc√™ √© um assistente educacional colaborativo inteligente.

SEU FLUXO DE TRABALHO (ReACT):

1. Quando receber uma PERGUNTA:
   - PRIMEIRO: Use 'consultar_BD' para buscar resumos similares (busca por embeddings, retorna top 3)
   
   - LEIA CUIDADOSAMENTE O RESULTADO:
     * Se come√ßar com "‚úÖ ENCONTRADO NO BANCO":
       ‚Üí Os resumos S√ÉO RELEVANTES (similaridade >= 0.6)
       ‚Üí RESPONDA IMEDIATAMENTE usando essas informa√ß√µes
       ‚Üí Informe ao usu√°rio que encontrou no banco de dados
       ‚Üí N√ÉO busque na internet
     
     * Se come√ßar com "‚ùå SIMILARIDADE BAIXA" ou "‚ùå BANCO DE DADOS VAZIO":
       ‚Üí Os resumos N√ÉO s√£o relevantes (similaridade < 0.6) ou n√£o existem
       ‚Üí Use 'buscar_referencias' IMEDIATAMENTE para buscar na internet
       ‚Üí Ap√≥s receber resultado, RESPONDA ao usu√°rio
   
   - SEMPRE responda na MESMA rodada ap√≥s usar as ferramentas
   - N√ÉO diga que "vai buscar" sem realmente buscar

2. Quando o usu√°rio pedir para GERAR RESUMO:
   - Use a ferramenta 'gerar_resumo' informando o tema da conversa
   - O sistema ir√° analisar TODO o hist√≥rico desta conversa
   - Gerar um resumo consolidado com LLM
   - Automaticamente salvar no banco com embedding

3. Intera√ß√£o Colaborativa:
   - Trabalhe com o usu√°rio para esclarecer d√∫vidas
   - Refine respostas conforme feedback
   - Mantenha conversas naturais e educativas

REGRAS CR√çTICAS:
- A ferramenta consultar_BD retorna ‚úÖ quando encontra (score >= 0.6) ou ‚ùå quando n√£o encontra (score < 0.6)
- CONFIE nos s√≠mbolos ‚úÖ e ‚ùå - Sentence-BERT √© MUITO preciso!
- Se viu ‚ùå, voc√™ DEVE usar buscar_referencias
- Se viu ‚úÖ, voc√™ N√ÉO DEVE buscar na internet, apenas responda
- NUNCA diga "vou buscar" e depois n√£o busque
- consultar_BD usa EMBEDDINGS (similaridade sem√¢ntica, n√£o busca exata de palavras)
- N√£o invente informa√ß√µes - use as ferramentas!""")
    
    response = model.invoke([system_prompt] + state["messages"])
    return {"messages": [response]}


def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """
    Decide se deve chamar ferramentas ou finalizar.
    """
    last_message = state["messages"][-1]
    
    # Se a √∫ltima mensagem tem chamadas de ferramentas, execut√°-las
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tools"
    else:
        return "end"


# =============== CONSTRU√á√ÉO DO GRAFO ===============

def create_agent_graph():
    """Cria e retorna o grafo compilado do agente ReACT."""
    
    # Cria o grafo
    workflow = StateGraph(AgentState)
    
    # Adiciona n√≥s
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", ToolNode(tools=tools))
    
    # Define o ponto de entrada
    workflow.set_entry_point("agent")
    
    # Adiciona arestas condicionais
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "end": END
        }
    )
    
    # Ap√≥s executar ferramentas, volta para o agente
    workflow.add_edge("tools", "agent")
    
    # Compila o grafo
    return workflow.compile()


# Cria a inst√¢ncia do agente
agent = create_agent_graph()


# =============== FUN√á√ÉO AUXILIAR ===============

def run_agent(user_input: str, conversation_history: list = None) -> dict:
    """
    Executa o agente com uma entrada do usu√°rio.
    
    Args:
        user_input: Mensagem do usu√°rio
        conversation_history: Hist√≥rico de mensagens anteriores
        
    Returns:
        Dict com a resposta e o hist√≥rico atualizado
    """
    if conversation_history is None:
        conversation_history = []
    
    # Adiciona a mensagem do usu√°rio
    conversation_history.append(HumanMessage(content=user_input))
    
    # Cria o estado inicial
    initial_state = {
        "messages": conversation_history
    }
    
    # Executa o agente
    result = agent.invoke(initial_state)
    
    # Retorna resultado e hist√≥rico atualizado
    return {
        "response": result["messages"][-1].content,
        "history": result["messages"]
    }